{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "#train the baseline CNN model for MNIST. Model is identical to that used in the tensorflow\n",
    "#tutorial: https://www.tensorflow.org/versions/master/tutorials/mnist/pros/index.html\n",
    "\n",
    "import mnist_input\n",
    "import mnist_model\n",
    "reload(mnist_model)\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "mnist = mnist_input.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"/home/justin/Programming/AdverserialMNIST/saved_models/baseline.ckpt\"\n",
    "x, y_ = mnist_model.place_holders()\n",
    "y_conv, keep_prob, variable_dict = mnist_model.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_entropy = -tf.reduce_sum(y_*tf.log(y_conv)) #loss function\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv,1), tf.argmax(y_,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "saver = tf.train.Saver(variable_dict)\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "saver.restore(sess, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.98\n",
      "step 1000, training accuracy 1\n",
      "step 2000, training accuracy 1\n",
      "step 3000, training accuracy 1\n",
      "step 4000, training accuracy 0.98\n",
      "step 5000, training accuracy 1\n",
      "step 6000, training accuracy 1\n",
      "step 7000, training accuracy 1\n",
      "step 8000, training accuracy 1\n",
      "step 9000, training accuracy 1\n",
      "step 10000, training accuracy 1\n",
      "step 11000, training accuracy 1\n",
      "step 12000, training accuracy 1\n",
      "step 13000, training accuracy 1\n",
      "step 14000, training accuracy 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(15000):\n",
    "  batch = mnist.train.next_batch(50)\n",
    "  if i%1000 == 0: #every 1000 batches we save the model and output the train accuracy\n",
    "    saver.save(sess, checkpoint_path)\n",
    "    train_accuracy = accuracy.eval(feed_dict={\n",
    "        x:batch[0], y_: batch[1], keep_prob: 1.0})\n",
    "    print(\"step %d, training accuracy %g\"%(i, train_accuracy))\n",
    "  train_step.run(feed_dict={x: batch[0], y_: batch[1], keep_prob: 0.5})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test accuracy: 0.991800\n"
     ]
    }
   ],
   "source": [
    "#compute accuracy on test set\n",
    "#my GPU gives a memory error if I try to run accuracy on the whole test set\n",
    "#so we feed in the test set in batches and aggregate the results\n",
    "idx = 0\n",
    "batch_size = 500\n",
    "num_correct = 0\n",
    "while(idx < len(mnist.test.images)):\n",
    "    num_correct += np.sum(correct_prediction.eval(feed_dict = {\n",
    "           x: mnist.test.images[idx:idx+batch_size], \n",
    "           y_: mnist.test.labels[idx:idx+batch_size], keep_prob: 1.0\n",
    "                }))\n",
    "    idx+=batch_size\n",
    "print \"test accuracy: %f\" %(float(num_correct)/float(len(mnist.test.images)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define the l1 norm of all model parameters\n",
    "absolute_sums = []\n",
    "for variable in variable_dict.values():\n",
    "    absolute_sums.append(tf.reduce_sum(tf.abs(variable)))\n",
    "l1_sum = tf.add_n(absolute_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "239372.95"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#total weight of all of the parameters in the model (for comparison with the l1-regularized model)\n",
    "sess.run(l1_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
